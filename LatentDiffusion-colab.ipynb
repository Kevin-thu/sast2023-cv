{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed0197ef",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/mikonvergence/DiffusionFastForward/blob/master/04-Latent-Diffusion-colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e50b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6073b910",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch lightning==2.0.5 transformers diffusers einops torchvision numpy matplotlib imageio scikit-image kornia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce528bb2",
   "metadata": {},
   "source": [
    "# Latent Diffusion Training\n",
    "\n",
    "In this notebook, we will train a simple `LatentDiffusion` model.\n",
    "\n",
    "The training should take up to 20 hours for reasonable results.\n",
    "\n",
    "Ideally, you will download this dataset once and store it as `./afhq`. If you're running on colab, it's a good idea to download it once to your personal machine (it's only 240 MB) and then upload it to your colab space when you start a new notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7d6dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./LatentDiffusion/')\n",
    "\n",
    "import os, sys, argparse\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import seed_everything\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "\n",
    "from src import *\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (8, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ac8810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Actually it's totally unnecessay and silly to use argparse in jupyter notebook. \n",
    "# But anyway, it's a good chance to do some practice and it will definitey be useful someday.\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--image_size\", type=int, default=256, help=\"Image size\")\n",
    "parser.add_argument(\"--train_dataset\", type=str, default=\"./afhq/\", help=\"The path to your training dataset\")\n",
    "parser.add_argument(\"--device\", type=int, default=0 if torch.cuda.is_available() else \"cpu\", help=\"Device number.\")\n",
    "parser.add_argument(\"--num_workers\", type=int, default=0, help=\"Spawn how many processes to load data.\")\n",
    "parser.add_argument(\"--seed\", type=int, default=42, help='manual seed')\n",
    "parser.add_argument(\"--max_epochs\", type=int, default=1000, help=\"Max epoch number to run.\")\n",
    "parser.add_argument(\"--ckpt_path\", type=str, default=\"\", help=\"Checkpoint path to load.\")\n",
    "parser.add_argument(\"--save_path\", type=str, default=\"./ckpt/\", help=\"Checkpoint path to save.\")\n",
    "parser.add_argument(\"--save_freq\", type=int, default=1, help=\"Save model every how many epochs.\")\n",
    "parser.add_argument(\"--ddim_steps\", type=int, default=50, help=\"DDIM timesteps\")\n",
    "# TODO begin: Add arguments lr and batch_size. It's recommended to set default lr to 1e-4 and default batch_size to 8.\n",
    "\n",
    "# TODO end\n",
    "args = parser.parse_args()\n",
    "seed_everything(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422ec7fb",
   "metadata": {},
   "source": [
    "### Prepare dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effe3e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kornia.utils import image_to_tensor\n",
    "import kornia.augmentation as KA\n",
    "\n",
    "class SimpleImageDataset(Dataset):\n",
    "    \"\"\"Dataset returning images in a folder.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 root_dir,\n",
    "                 transforms = None):\n",
    "\n",
    "        self.root_dir = root_dir\n",
    "        self.transforms = transforms\n",
    "\n",
    "        # set up transforms\n",
    "        if self.transforms is not None:\n",
    "            data_keys = ['input']\n",
    "\n",
    "            self.input_T = KA.container.AugmentationSequential(\n",
    "                *self.transforms,\n",
    "                data_keys = data_keys,\n",
    "                same_on_batch = False\n",
    "            )\n",
    "\n",
    "        # TODO begin: Define the image paths filtered by the `supported_formats` in your datasets\n",
    "        # Hint: os.listdir\n",
    "        # Challenge: Can you complete this task in one line? (hint: Python comprehension, refer to Python basics handout by Yifan Li)\n",
    "        supported_formats = [\"jpg\", \"png\"]\n",
    "        self.image_names = None\n",
    "        # TODO end\n",
    "\n",
    "    def __len__(self):\n",
    "        # TODO begin: Return the length of your dataset\n",
    "        return 0\n",
    "        # TODO end\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(self.root_dir, self.image_names[idx])\n",
    "        image = image_to_tensor(imageio.imread(img_name)) / 255\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            image = self.input_T(image)[0]\n",
    "\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528e3f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "\n",
    "CROP_SIZE = args.image_size\n",
    "\n",
    "transform = [\n",
    "    KA.RandomCrop((2 * CROP_SIZE,2 * CROP_SIZE)),\n",
    "    KA.Resize((CROP_SIZE, CROP_SIZE), antialias=True),\n",
    "    KA.RandomVerticalFlip()\n",
    "  ]\n",
    "\n",
    "train_dataset = SimpleImageDataset(args.train_dataset, transforms = transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4534f313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO begin: Define the training dataloader using torch.utils.data.DataLoader\n",
    "# Hint: check the API of torch.utils.data.DataLoader, especially arguments like batch_size, shuffle, num_workers\n",
    "train_dataloader = None\n",
    "# TODO end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e372cc",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04156f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import *\n",
    "\n",
    "# TODO begin: complete the LatentDiffusion Model in `src`\n",
    "model = LatentDiffusion(lr = args.lr, batch_size = args.batch_size)\n",
    "# TODO end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92f33e5",
   "metadata": {},
   "source": [
    "...but first, let's check if the used `AutoEncoder` (`model.vae`) can reconstruct our samples well.\n",
    "\n",
    "**You should always test your autoencoder in this way when using latent diffusion models on a new dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786b43c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = train_dataset[0]\n",
    "\n",
    "# TODO begin: Show the example img and use vae to reconstruct it using matplotlib\n",
    "# Hint: plt.imshow\n",
    "# Challenge: What's the image shape here? Should you permute or unsqueeze it?\n",
    "plt.subplot(1,2,1)\n",
    "# Plot the original img here\n",
    "\n",
    "plt.title('Input')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "# Plot the reconstructed img by `mode.vae` here\n",
    "\n",
    "plt.title('AutoEncoder Reconstruction')\n",
    "# TODO end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a83cab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the trainer using PyTorch Lightning\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(dirpath=args.save_path, every_n_epochs=args.save_freq)\n",
    "\n",
    "# TODO: You can specify other parameters here, like accelerator, devices...\n",
    "# You can check the pl.Trainer API here: https://lightning.ai/docs/pytorch/stable/common/trainer.html\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs = args.max_epochs,\n",
    "    callbacks = [EMA(0.9999), checkpoint_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deafb040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Easy to train the model in PyTorch Lightning in one line\n",
    "trainer.fit(model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader, ckpt_path=args.ckpt_path if args.ckpt_path else None)\n",
    "# TODO Challenge: Can you add some logging and visualization codes to better babysitting the training process? \n",
    "# Hint: There are many librarys you can use, e.g. logging, tensorboard, wandb... And the easiest way: print the loss every step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789d9c71",
   "metadata": {},
   "source": [
    "Go to sleep now ~ This one line might run for days...\n",
    "\n",
    "Wait! Please make sure that you have save the checkpoints correctly. \n",
    "\n",
    "If the code breaks for some reason, you can load the checkpoint and continue training.\n",
    "\n",
    "### Now sample images from your model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4faf7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(args.device)\n",
    "out = model(batch_size = args.batch_size, shape = (64,64), verbose = True)\n",
    "# You can also try `sampler=DDIM_Sampler(num_steps=args.ddim_steps)`, which can sample images in less than 50 steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeddad22",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(out.shape[0]):\n",
    "    plt.subplot(1,len(out),idx+1)\n",
    "    plt.imshow(out[idx].detach().cpu().permute(1,2,0))\n",
    "    plt.axis('off')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ldm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
